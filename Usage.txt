1) Processing the STARD data:

C:\Users\Tejas\Documents\qlesq_project\qlesq_prediction>python code/data-cleaning/stard_preprocessing_manager.py data/STARD_raw -a


2) Processing the CANBIND data:

C:\Users\Tejas\Documents\qlesq_project\qlesq_prediction>python code/data-cleaning/canbind_preprocessing_manager.py

note: before running this, go into canbind_preprocessing_manager.py and alter pathData to a correct pathing for the raw data (see below; will be different depending on the user)

    elif len(sys.argv) == 1:
        pathData = r'C:\Users\Tejas\Documents\qlesq_project\qlesq_prediction\data\canbind_raw_data'
        aggregate_and_clean(pathData, verbose=False)
        ygen(pathData)
        impute(pathData)
        convert_canbind_to_overlapping(pathData)


3) Create Overlapping Data from processed data:

C:\Users\Tejas\Documents\qlesq_project\qlesq_prediction>python code/data-cleaning/generate_overlapping_features.py -both data/STARD_raw/processed_data/final_xy_data_matrices data/canbind_raw_data/

4) Create y targets:

python code/create_targets.py data/STARD_raw/processed_data/final_xy_data_matrices/y_qlesq_77__final

python code/create_targets.py data/STARD_raw/processed_data/final_xy_data_matrices/y_qlesq_91__final

python code/create_targets.py data/canbind_raw_data/q-les-q/canbind_qlesq_y

5) Move Relevant Datasets to modelling folder:
python code/move_modelling.py

6) Create Train/Test Data:

python code/train_test_split.py X_77_qlesq_sr__final_extval.csv y_qlesq_77__final__targets.csv 

python code/train_test_split.py X_77_qlesq_sr__final.csv y_qlesq_77__final__targets.csv 

7) Optional: Running a GridSearch (before running, check parameter grids and adjust as needed)


python code/grid_search.py [model_type] [data_type] 

model_type has several options:
- rf --> random forest
- lr --> logistic regression
- gbdt --> gradient boosting classifiers
- svc  --> support vector classifier
- knn --> k nearest neighbors

data_type has 3 options:
- full --> full feature set (480)
- over --> overlapping feature set (100)
- rfe --> recursive feature elimination set
- enet --> elastic net CV feature selection


8) Running an experiment: (can't remember if this is redundant or not...)

Experiments are run via run_experiment.py. It utilizes classes so code in main() will need to be altered to contain models of your choice. The type of data is given as arguements to main().

9) Run an Evaluation on STARD and CANBIND holdout sets:

There are three options:
- Full feature evalution on STARD holdout   (eval_type ="full")
- Full fetaures w/ enet selection (eval_type = "full_enet")
- Full features w/ recursive elimination selection (eval_type = "full_rfe")
- Overlapping feature evaluation on STARD holdout (eval_type = "over")
- Overlapping feature evaluation on CANBIND holdout (eval_type = "canbind")

python code/run_evaluation.py [eval_type] [a name for the evaluation]


Optional Scripts:

a) RFE Selection

There are two options for eval type:
- Full features   (eval_type ="full")

- Overlapping features  (eval_type = "over")


Usage: python code/rfe_selection.py [eval_type] [rfe_step] [n_feats to select]
ex. python code/rfe_selection.py full 1 30

b) Elastic Net Selection


